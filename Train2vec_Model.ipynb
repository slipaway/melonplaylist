{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train2vec Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6h0f36mP89N6PK7YsZSSU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slipaway/melonplaylist/blob/master/Train2vec_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xglmAVfp0js1",
        "colab_type": "text"
      },
      "source": [
        "train data 전처리 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jId3ZtZ30fb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/kakao/khaiii.git\n",
        "!pip install cmake\n",
        "!mkdir build\n",
        "!cd build && cmake /content/khaiii\n",
        "!cd /content/build/ && make all\n",
        "!cd /content/build/ && make resource\n",
        "!cd /content/build && make install\n",
        "!cd /content/build && make package_python\n",
        "!pip install /content/build/package_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cdfa_ek1WdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz1rF6KS1XXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from datetime import timedelta, datetime\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import *\n",
        "from khaiii import KhaiiiApi\n",
        "from collections import Counter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztkfew2D1bQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder = '/content/gdrive/My Drive/melon'\n",
        "filelist = os.chdir(folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTEkEOOp1irn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre_gn_all=pd.read_json('genre_gn_all.json', typ='series')\n",
        "train=pd.read_json('train.json')\n",
        "song_meta=pd.read_json('song_meta.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3NxXYCb1jmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/slipaway/melonplaylist.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAkLgAJ-1mqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre_gn_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N90_e4n11nXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre_gn_all = pd.DataFrame(genre_gn_all, columns = ['gnr_name']).reset_index().rename(columns = {'index' : 'gnr_code'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xfawjDV1pQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gnr_code=genre_gn_all[genre_gn_all['gnr_code'].str[-2:]=='00']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnSAVY7b1vjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#대장르 장르코드 리스트\n",
        "gnr_code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN__9R7s1yV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#곡 아이디(id)와 대분류 장르코드 리스트(song_gn_gnr_basket) 추출\n",
        "\n",
        "song_gnr_map = song_meta.loc[:, ['id', 'song_gn_gnr_basket']]\n",
        "\n",
        "#unnest song_gn_gnr_basket\n",
        "song_gnr_map_unnest = np.dstack(\n",
        "    (\n",
        "        np.repeat(song_gnr_map.id.values, list(map(len, song_gnr_map.song_gn_gnr_basket))),\n",
        "        np.concatenate(song_gnr_map.song_gn_gnr_basket.values)\n",
        "    )\n",
        ")\n",
        "\n",
        "#unnested 데이터프레임 생성 : song_gnr_map\n",
        "\n",
        "song_gnr_map = pd.DataFrame(data = song_gnr_map_unnest[0], columns=song_gnr_map.columns)\n",
        "song_gnr_map['id']=song_gnr_map['id'].astype(str)\n",
        "song_gnr_map.rename(columns={'id' : 'song_id', 'song_gn_gnr_basket' : 'gnr_code'}, inplace=True)\n",
        "\n",
        "#unnest 객체 제거\n",
        "del song_gnr_map_unnest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcJCfJbb160g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plylst_date = train[['updt_date', 'songs']]\n",
        "plylst_date_unnest = np.dstack(\n",
        "    (\n",
        "        np.repeat(plylst_date.updt_date.values, list(map(len, plylst_date.songs))), \n",
        "        np.concatenate(plylst_date.songs.values)\n",
        "    )\n",
        ")\n",
        "plylst_date = pd.DataFrame(data=plylst_date_unnest[0], columns = plylst_date.columns)\n",
        "\n",
        "del plylst_date_unnest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx_vSQIU19aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plylst_date[\"updt_date\"] = plylst_date[\"updt_date\"].map(lambda x: x[:4] + x[5:7] + x[8:10])\n",
        "song_date = song_meta[[\"id\",\"issue_date\"]]\n",
        "plylst_song_date = pd.merge(plylst_date, song_date, left_on=\"songs\", right_on=\"id\", how='left')\n",
        "plylst_song_date[\"issue_date\"] = plylst_song_date[\"issue_date\"].astype(str)\n",
        "\n",
        "plylst_song_date[\"strange\"] = plylst_song_date[\"updt_date\"] < plylst_song_date[\"issue_date\"]\n",
        "strange_songs = plylst_song_date[plylst_song_date[\"strange\"] == True].drop('songs', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1g3ldno1iUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDdFx2gJ2B8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs=strange_songs.drop_duplicates(['id'])\n",
        "len(strange_songs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byM7JEbi2EM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs_list=strange_songs['id'].values.astype(str)\n",
        "strange_song_gnr_map=song_gnr_map[song_gnr_map['song_id'].isin(strange_songs_list)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa6qY6rw2HRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(strange_songs_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkMgd5I-2JPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "song_gnr_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QWC_Tuc2Ln9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_song_gnr_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2jFNT7q2OL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_cnt=strange_song_gnr_map.groupby('gnr_code').count()\n",
        "strange_cnt=pd.merge(strange_cnt, gnr_code, on='gnr_code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsVZMx7o2Q9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCa3NF4j2T82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs=strange_songs.drop_duplicates(['id'])\n",
        "print(strange_songs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0VogwC-2WXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_id=strange_songs.loc[:, 'id']\n",
        "strange_id\n",
        "strange_id_list=list(strange_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N0ZWDZl2X2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_id=train.loc[:, 'id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6CbJD1A2Zry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "song_meta_id=song_meta.loc[:, 'id']\n",
        "print(song_meta_id.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e53Aw7zD2cGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNF3g5HW2fKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트 아이디(id)와 수록곡(songs) 추출\n",
        "plylst_song_map=train[['id', 'songs']]\n",
        "\n",
        "#unnest songs\n",
        "\n",
        "plylst_song_map_unnest=np.dstack(\n",
        "    (\n",
        "        np.repeat(plylst_song_map.id.values, list(map(len, plylst_song_map.songs))),\n",
        "        np.concatenate(plylst_song_map.songs.values)\n",
        "    )\n",
        ")\n",
        "\n",
        "#unnested 데이터프레임 생성 : plylst_song_map\n",
        "plylst_song_map=pd.DataFrame(data=plylst_song_map_unnest[0], columns=plylst_song_map.columns)\n",
        "plylst_song_map['id']=plylst_song_map['id'].astype(str)\n",
        "plylst_song_map['songs']=plylst_song_map['songs'].astype(str)\n",
        "\n",
        "del plylst_song_map_unnest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-p-DHTe2jhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트에 수록된 곡의 총 개수(중복 포함)\n",
        "\n",
        "len(plylst_song_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6syEPt72lty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트에 strange 곡 제거한 후, 수록된 곡의 총 개수(중복 포함)\n",
        "\n",
        "plylst_song_map_remove=plylst_song_map[np.logical_not(plylst_song_map.songs.isin(strange_songs_list))]\n",
        "plylst_song_map_remove.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Pgxx8wE2nJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plylst_song_map_remove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKnQ8M-82q81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train set에 새로운 column : remove_songs 생성\n",
        "#remove_songs : strange 곡 제거한 후 다시 만든 플레이리스트\n",
        "\n",
        "plylst_song_group=plylst_song_map_remove.groupby('id')['songs'].apply(list).reset_index(name='remove_songs')\n",
        "train['id']=train['id'].astype(str)\n",
        "train_remove = pd.merge(train, plylst_song_group)\n",
        "\n",
        "train_remove.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO6NctFQ2vPv",
        "colab_type": "text"
      },
      "source": [
        "train data에서 tag와 playlist title을 형태소 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15xQE9iz298a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def re_sub(series: pd.Series) -> pd.Series:\n",
        "  series = series.str.replace(pat=r'[ㄱ-ㅎ]', repl=r'', regex=True) #ㅋ 제거용\n",
        "  series = series.str.replace(pat=r'[^\\w\\s]', repl=r'', regex=True) #특수문자 제거용\n",
        "  series = series.str.replace(pat=r'[ ]{2,}', repl=r' ', regex=True) #공백 제거\n",
        "  series = series.str.replace(pat=r'[\\u3000]+', repl=r'', regex=True) #u3000제거\n",
        "  return series\n",
        "\n",
        "def flatten(list_of_list : List) -> List:\n",
        "  flatten = [j for i in list_of_list for j in i]\n",
        "  return flatten\n",
        "\n",
        "def get_token(title: str, tokenizer)-> List[Tuple]:\n",
        "\n",
        "  if len(title)==0 or title== ' ' : #제목이 공백인 경우 tokenizer 에러 발생\n",
        "    return []\n",
        "  \n",
        "  result = tokenizer.analyze(title)\n",
        "  result = [(morph.lex, morph.tag) for split in result for morph in split.morphs] # (형태소, 품사) 튜플의 리스트\n",
        "  return result\n",
        "\n",
        "def get_all_tags(df) -> List:\n",
        "  tag_list = df['tags'].values.tolist()\n",
        "  tag_list=flatten(tag_list)\n",
        "  return tag_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNBo5sw13App",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = KhaiiiApi()\n",
        "all_tag = get_all_tags(train)\n",
        "token_tag = [get_token(x, tokenizer) for x in all_tag] #태그를 형태소 분석"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHRcQ8wL3C3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_tag[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMtXOrmr3EwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_itself=list(filter(lambda x: len(x)==1, token_tag)) # 태그 자체가 형태소여서 분리되지 않는 태그만 고름\n",
        "token_itself=flatten(token_itself)\n",
        "flatten_token=flatten(token_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFTi1nFe3RHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('%-23s'%'# of original tag is', f'{len(all_tag):8,}')\n",
        "print('%-23s'%'# of morpheme itself is', f'{len(token_itself):8,}')\n",
        "print('%-23s'%'# of total token is', f'{len(flatten_token):8,}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_Z-o28A3TEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#counting part of speech (pos)\n",
        "pos=[x[1] for x in token_itself]\n",
        "pos_count=Counter(pos)\n",
        "popular_pos = pos_count.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6VlV8a83VTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tag 분류표\n",
        "objects=[x[0] for x in popular_pos]\n",
        "y_pos=np.arange(len(objects))\n",
        "performance=[x[1] for x in popular_pos]\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Usage')\n",
        "plt.title('Part of Speech - Tags')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWYcmBz73Z2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트 제목 형태소 분석\n",
        "\n",
        "train['plylst_title']=re_sub(train['plylst_title'])\n",
        "train.loc[:, 'ply_token']=train['plylst_title'].map(lambda x: get_token(x, tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjJljL8a3bGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "using_pos=['NNG', 'SL', 'NNP', 'MAG', 'SN'] #일반 명사, 외국어, 고유 명사, 일반 부사, 숫자\n",
        "train['ply_token']=train['ply_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoCwCoXW3ftY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_tag=set(token_itself)\n",
        "unique_word=[x[0] for x in unique_tag]\n",
        "\n",
        "#train 데이터의 plylst title 형태소 분리"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-OKcii83i20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#정답 tag에 나온 형태소만 남겨두기\n",
        "train['ply_token']=train['ply_token'].map(lambda x: list(filter(lambda x: x[0] in unique_word, x)))\n",
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65WJh6A53ok8",
        "colab_type": "text"
      },
      "source": [
        "형태소 분석한 tag, playlist title과 날짜가 맞지 않는 노래들을 제외한 목록이 추가된 train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKUI5Al03rA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_eda = pd.merge(train, plylst_song_group)\n",
        "train_eda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3StZ3jd3vv0",
        "colab_type": "text"
      },
      "source": [
        "word2vec 학습을 위해 사용할 데이터들만 모아 놓은 dataframe 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOrODbqE3s4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wv_sent = train_eda[['ply_token', 'tags', 'remove_songs']]\n",
        "wv_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFXxyBxg3-kc",
        "colab_type": "text"
      },
      "source": [
        "플레이리스트 제목을 형태소 분해한 데이터에서 품사정보를 제외"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGUIhuxf2jQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word2vec 이용을 위해 ply_token data를 전처리\n",
        "z = []\n",
        "y = []\n",
        "for i in range(len(wv_sent['ply_token'])):\n",
        "  z = []\n",
        "  for j in range(len(wv_sent['ply_token'][i])):\n",
        "    x = wv_sent['ply_token'][i][j]\n",
        "    x = list(x)\n",
        "    del x[1]\n",
        "    z.insert(j,\"\".join(x))\n",
        "  y.insert(i,z)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78HkrjGg4E0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wv_sent['ply_token_new'] = y\n",
        "wv_sent = wv_sent[['ply_token_new', 'tags', 'remove_songs']]\n",
        "wv_sent # Word2vec 이용을 위해 필요한 data들만 모아둔 dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmy0PRW54LCa",
        "colab_type": "text"
      },
      "source": [
        "train data에 존재하는 플레이리스트 제목을 형태소 분해한 것들의 리스트\n",
        "& train data에 존재하는 태그들을 형태소 분해한 것들의 리스트\n",
        "& train data에 존재하는 노래들의 리스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_2wlq3e4MP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_token_list = np.dstack(\n",
        "    (\n",
        "        np.concatenate(wv_sent.ply_token_new.values)\n",
        "    )\n",
        ")\n",
        "train_token_list = pd.DataFrame(train_token_list[0][0], columns = ['token'])\n",
        "train_token_list = list(train_token_list['token']) # train data에 존재하는 모든 플레이리스트 제목 형태소들의 list\n",
        "\n",
        "train_tag_list = np.dstack(\n",
        "    (\n",
        "        np.concatenate(wv_sent.tags.values)\n",
        "    )\n",
        ")\n",
        "train_tag_list = pd.DataFrame(train_tag_list[0][0], columns = ['tag'])\n",
        "train_tag_list = list(train_tag_list['tag']) # train data에 존재하는 모든 태그들의 list\n",
        "\n",
        "train_song_list = np.dstack(\n",
        "    (\n",
        "        np.concatenate(wv_sent.remove_songs.values)\n",
        "    )\n",
        ")\n",
        "train_song_list = pd.DataFrame(train_song_list[0][0], columns = ['song'])\n",
        "train_song_list = list(train_song_list['song']) # train data에 존재하는 노래들의 list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68A7U8EX4Ew1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_token_list_unduplicated = list(set(train_token_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgjB6lV4VCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tag_list_unduplicated = list(set(train_tag_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vNFk96R4Wsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_song_list_unduplicated = list(set(train_song_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohi-vJu28APp",
        "colab_type": "text"
      },
      "source": [
        "Word2vec 사용을 위해 플레이리스트 별로 제목의 형태소와 태그, 곡 id를 한 문장으로 갖도록 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyEQu5WL8FjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word2vec 학습을 위해 플레이리스트 별로 제목의 형태소와 태그, 곡 id를 한 문장으로 갖도록 전처리\n",
        "w = []\n",
        "for i in range(len(wv_sent)):\n",
        "  w.insert(i,wv_sent['ply_token_new'][i] + wv_sent['tags'][i] + wv_sent['remove_songs'][i])\n",
        "train_eda_fin = w\n",
        "train_eda_fin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ian5HSOa8SMq",
        "colab_type": "text"
      },
      "source": [
        "word2vec 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4A4vwOc8I10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "class EpochSaver(CallbackAny2Vec):\n",
        "     '''Callback to save model after each epoch.'''\n",
        "\n",
        "     def __init__(self, path_prefix):\n",
        "         self.path_prefix = path_prefix\n",
        "         self.epoch = 0\n",
        "\n",
        "     def on_epoch_end(self, model):\n",
        "         output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n",
        "         model.save(output_path)\n",
        "         self.epoch += 1\n",
        "\n",
        "class EpochLogger(CallbackAny2Vec):\n",
        "     def __init__(self):\n",
        "         self.epoch = 0\n",
        "\n",
        "     def on_epoch_begin(self, model):\n",
        "         print(\"Epoch #{} start\".format(self.epoch))\n",
        "\n",
        "     def on_epoch_end(self, model):\n",
        "         print(\"Epoch #{} end\".format(self.epoch))\n",
        "         self.epoch += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zduxj5H4EtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "epoch_logger = EpochLogger()\n",
        "wv_model = Word2Vec(train_eda_fin, window = 10, min_count=7, workers=6, size = 400, iter=50, sg=1, callbacks=[epoch_logger])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaKaO4Wy4Eow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_list = wv_model.wv.vocab.keys()\n",
        "# word2vec에 의해 학습된 것들(플레이 리스트 제목이 형태소로 분해된 것들 & 태그 & 곡 id)의 리스트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSxZqIXT8gIJ",
        "colab_type": "text"
      },
      "source": [
        "test data 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8f2wgSz8a5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=pd.read_json('test.json')\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVwJWIxp8lGE",
        "colab_type": "text"
      },
      "source": [
        "test data 중에서 일부 곡과 일부 태그가 존재하는 경우를 제외한 data set 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RMwgEga8mJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "me = []\n",
        "for i in range(len(test['id'])):\n",
        "  if len(test['plylst_title'][i]) == 0 and len(test['tags'][i]) != 0 and len(test['songs'][i]) !=0:\n",
        "    me.append(i)\n",
        "  else:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4CDfJdM8tmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = test.drop(me, axis = 0)\n",
        "test = test.reset_index(drop=True)\n",
        "test # 곡과 태그만 존재하는 플레이리스트를 제외한 나머지 플레이리스트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Suud8583DO",
        "colab_type": "text"
      },
      "source": [
        "위의 결과로 얻은 데이터셋 중에서 곡만 존재하는 플레이리스트를 제외한 데이터셋 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCyzWtuf8wLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mel = []\n",
        "for i in range(len(test['id'])):\n",
        "  if len(test['plylst_title'][i]) == 0 and len(test['tags'][i]) == 0 and len(test['songs'][i]) !=0:\n",
        "    mel.append(i)\n",
        "  else:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LygJHDbK81he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1 = test.drop(mel, axis = 0)\n",
        "test_1 = test_1.reset_index(drop=True)\n",
        "test_1 # 곡과 태그만 존재하는 플레이리스트를 제외한 나머지 플레이리스트 중에서 곡만 존재하는 플레이리스트를 제외 -> 제목+태그 or 제목만 있는 경우"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1vPHMd29ZyB",
        "colab_type": "text"
      },
      "source": [
        "test_1 data의 제목과 태그에 대해 형태소 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqWHIu9J9fiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def re_sub(series: pd.Series) -> pd.Series:\n",
        "  series = series.str.replace(pat=r'[ㄱ-ㅎ]', repl=r'', regex=True) #ㅋ 제거용\n",
        "  series = series.str.replace(pat=r'[^\\w\\s]', repl=r'', regex=True) #특수문자 제거용\n",
        "  series = series.str.replace(pat=r'[ ]{2,}', repl=r' ', regex=True) #공백 제거\n",
        "  series = series.str.replace(pat=r'[\\u3000]+', repl=r'', regex=True) #u3000제거\n",
        "  return series\n",
        "\n",
        "def flatten(list_of_list : List) -> List:\n",
        "  flatten = [j for i in list_of_list for j in i]\n",
        "  return flatten\n",
        "\n",
        "def get_token(title: str, tokenizer)-> List[Tuple]:\n",
        "\n",
        "  if len(title)==0 or title== ' ' : #제목이 공백인 경우 tokenizer 에러 발생\n",
        "    return []\n",
        "  \n",
        "  result = tokenizer.analyze(title)\n",
        "  result = [(morph.lex, morph.tag) for split in result for morph in split.morphs] # (형태소, 품사) 튜플의 리스트\n",
        "  return result\n",
        "\n",
        "def get_all_tags(df) -> List:\n",
        "  tag_list = df['tags'].values.tolist()\n",
        "  tag_list=flatten(tag_list)\n",
        "  return tag_list\n",
        "\n",
        "tokenizer = KhaiiiApi()\n",
        "all_tag = get_all_tags(test_1)\n",
        "token_tag = [get_token(x, tokenizer) for x in all_tag] #태그를 형태소 분석\n",
        "\n",
        "token_tag[:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb-W5PUu9g8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_itself=list(filter(lambda x: len(x)==1, token_tag)) # 태그 자체가 형태소여서 분리되지 않는 태그만 고름\n",
        "token_itself=flatten(token_itself)\n",
        "flatten_token=flatten(token_tag)\n",
        "print('%-23s'%'# of original tag is', f'{len(all_tag):8,}')\n",
        "print('%-23s'%'# of morpheme itself is', f'{len(token_itself):8,}')\n",
        "print('%-23s'%'# of total token is', f'{len(flatten_token):8,}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5swouqn9l5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#counting part of speech (pos)\n",
        "pos=[x[1] for x in token_itself]\n",
        "pos_count=Counter(pos)\n",
        "popular_pos = pos_count.most_common()\n",
        "#tag 분류표\n",
        "objects=[x[0] for x in popular_pos]\n",
        "y_pos=np.arange(len(objects))\n",
        "performance=[x[1] for x in popular_pos]\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Usage')\n",
        "plt.title('Part of Speech - Tags')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EVQvJNy9nJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트 제목 형태소 분석\n",
        "test_1['plylst_title']=re_sub(test_1['plylst_title'])\n",
        "test_1.loc[:, 'ply_token']=test_1['plylst_title'].map(lambda x: get_token(x, tokenizer))\n",
        "using_pos=['NNG', 'SL', 'NNP', 'MAG', 'SN'] #일반 명사, 외국어, 고유 명사, 일반 부사, 숫자\n",
        "test_1['ply_token']=test_1['ply_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n",
        "unique_tag=set(token_itself)\n",
        "unique_word=[x[0] for x in unique_tag]\n",
        "#정답 tag에 나온 형태소만 남겨두기\n",
        "test_1['ply_token']=test_1['ply_token'].map(lambda x: list(filter(lambda x: x[0] in unique_word, x)))\n",
        "test_1.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZbmdVT8yxvt",
        "colab_type": "text"
      },
      "source": [
        "test_1 data에서 형태소 분석한 결과를 적용시킨 data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEn0b6RVy0M1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI2JzzNWy_vu",
        "colab_type": "text"
      },
      "source": [
        "전처리된 test_1 data에 대해 word2vec 적용을 위해 필요한 data들만 추출한 dataframe 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuRdIXwAzCBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent = test_1[['id', 'ply_token', 'tags']]\n",
        "test_1_sent = pd.DataFrame(test_1_sent)\n",
        "test_1_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Tc48Fn0_Nt",
        "colab_type": "text"
      },
      "source": [
        "전처리된 test_1 data 중 플레이리스트 제목을 형태소 분해한 data에서 품사정보를 삭제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNW9eIT71B6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word2vec 이용을 위해 ply_token data를 전처리\n",
        "z = []\n",
        "y = []\n",
        "for i in range(len(test_1_sent['ply_token'])):\n",
        "  z = []\n",
        "  if len(test_1_sent['ply_token'][i]) == 0:\n",
        "    x = test_1_sent['ply_token'][i]\n",
        "    z = x\n",
        "  else:\n",
        "    for j in range(len(test_1_sent['ply_token'][i])):\n",
        "     x = test_1_sent['ply_token'][i][j]\n",
        "     z.insert(j,x[0])\n",
        "  y.insert(i,z)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSLrQxpA1EIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent['ply_token'] = y\n",
        "test_1_sent # Word2vec 이용을 위해 필요한 data들만 모아둔 dataframe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkyAURe11G1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_1_sent에서 학습된 리스트에 존재하지 않는 ply_token을 제외함 (Word2vec 모델 적용을 위함)\n",
        "for i in range(len(test_1_sent['ply_token'])):\n",
        "  if len(test_1_sent['ply_token'][i]) == 0:\n",
        "    test_1_sent['ply_token'][i] = test_1_sent['ply_token'][i]\n",
        "  else:\n",
        "    a = pd.DataFrame(test_1_sent['ply_token'][i], columns = ['token'])\n",
        "    a['logical'] = a.isin(learning_list).astype(str)\n",
        "    b = a['logical'] == 'True' \n",
        "    if b.sum() == 0:\n",
        "      test_1_sent['ply_token'][i] = []\n",
        "    else:\n",
        "      c = c = a[b].drop('logical',axis=1)\n",
        "      c = c.token.tolist()\n",
        "      test_1_sent['ply_token'][i] = c\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfwM7FzE1NFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_1_sent에서 학습된 리스트에 존재하지 않는 tags를 제외함 (Word2vec 모델 적용을 위함)\n",
        "for i in range(len(test_1_sent['tags'])):\n",
        "  if len(test_1_sent['tags'][i]) == 0:\n",
        "    test_1_sent['tags'][i] = test_1_sent['tags'][i]\n",
        "  else:\n",
        "    a = pd.DataFrame(test_1_sent['tags'][i], columns = ['tag'])\n",
        "    a['logical'] = a.isin(learning_list).astype(str)\n",
        "    b = a['logical'] == 'True' \n",
        "    if b.sum() == 0:\n",
        "      test_1_sent['tags'][i] = []\n",
        "    else:\n",
        "      c = c = a[b].drop('logical',axis=1)\n",
        "      c = c.tag.tolist()\n",
        "      test_1_sent['tags'][i] = c\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbXO-ZaX2Xeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5ZYFSRd2qvm",
        "colab_type": "text"
      },
      "source": [
        "word2vec 적용을 위해 학습된 리스트에 존재하지 않는 data들을 제외하는 과정에서 input이 모두 사라져버린 data set들을 추출해내는 과정 -> Matrix Factorization 모델을 통해 태그를 채워서 다시 Word2vec 모델을 적용할 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy9kgMdH2sIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m1 = []\n",
        "for i in range(len(test_1_sent['id'])):\n",
        "  if len(test_1_sent['ply_token'][i]) == 0 and len(test_1_sent['tags'][i]) == 0:\n",
        "    m1.append(i)\n",
        "  else:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ustZakEk24-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent_zerolen = test_1_sent.loc[m1]\n",
        "test_1_sent_zerolen = test_1_sent_zerolen.reset_index(drop=True)\n",
        "test_1_sent_zerolen # test_1_sent 중 전처리결과 token, tag, song 모두 하나도 남지 않게 된 플레이리스트\n",
        "# 나중에 Matrix Factorization 모델을 통해 10개의 태그를 예측한 후 해당 태그를 바탕으로 Word2vec 모델을 적용할 것"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AV7RiFU321v",
        "colab_type": "text"
      },
      "source": [
        "word2vec 적용을 위해 학습된 리스트에 존재하지 않는 data들을 제외하는 과정에서 input이 모두 사라져버린 data set들을 제외한 dataframe 생성 -> 해당 data에 대해서는 word2vec 모델을 적용할 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpOIe93-36Xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent_excluded = test_1_sent.drop(m1, axis = 0)\n",
        "test_1_sent_excluded = test_1_sent_excluded.reset_index(drop=True)\n",
        "test_1_sent_excluded # test_1_sent 중 전처리결과 token, tag, song 모두 하나도 남지 않게 된 플레이리스트들을 제외한 플레이리스트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79JN1HsdzGAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word2vec 사용을 위해 플레이리스트 별로 제목의 형태소와 태그, 단어를 한 문장으로 갖도록 test_1_sent_excluded를 전처리\n",
        "w = []\n",
        "for i in range(len(test_1_sent_excluded)):\n",
        "  w.insert(i,test_1_sent_excluded['ply_token'][i] + test_1_sent_excluded['tags'][i])\n",
        "test_1_input_excluded = w\n",
        "test_1_input_excluded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UBk9Akx4S9l",
        "colab_type": "text"
      },
      "source": [
        "test_1_input_expected data에 대해 Word2vec 모델을 이용하여 input과 유사한 output을 500개 뽑아내는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f10cbRFy1-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similar_1_list(x):\n",
        "  aa = [wv_model.wv.most_similar(positive = x[i], topn = 500) for i in range(len(x))]\n",
        "  return aa\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWByKJAP4nj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xxxx_1 = similar_1_list(test_1_input_excluded)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgHINSa8423G",
        "colab_type": "text"
      },
      "source": [
        "word2vec model을 적용한 결과에서 유사도 data를 제외하고 유사한 data만을 남기는 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhNi0IEB45rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = []\n",
        "y_1 = []\n",
        "for i in range(len(xxxx_1)):\n",
        "  z = []\n",
        "  for j in range(len(xxxx_1[i])):\n",
        "    x5_1 = xxxx_1[i][j]\n",
        "    z.insert(j,x5_1[0])\n",
        "  y_1.insert(i,z)\n",
        "y_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy4O6nmT5Bpu",
        "colab_type": "text"
      },
      "source": [
        "word2vec model을 적용한 결과가 태그, 제목 형태소, 곡 id들이 섞여서 나오도록 모델링이 되어있으므로, train song list에 해당하는 곡과 train tag list에 해당하는 태그를 구분해야 할 필요 존재 -> 이를 위한 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZtVH0mb5J4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isinsonglist(m):\n",
        "  return m in train_song_list_unduplicated\n",
        "\n",
        "def isintaglist(e):\n",
        "  return e in train_tag_list_unduplicated\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_y7h1pZ5Nsz",
        "colab_type": "text"
      },
      "source": [
        "test_1_sent_excluded에 predicted_songs와 predicted_tags column 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf9D7O5q9jsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_songs = []\n",
        "for i in range(len(test_1_sent_excluded)):\n",
        "  predicted_songs.append([])\n",
        "\n",
        "test_1_sent_excluded['predicted_songs'] = predicted_songs\n",
        "\n",
        "predicted_tags = []\n",
        "for i in range(len(test_1_sent_excluded)):\n",
        "  predicted_tags.append([])\n",
        "\n",
        "test_1_sent_excluded['predicted_tags'] = predicted_tags\n",
        "\n",
        "test_1_sent_excluded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGwJRGbn6Jro",
        "colab_type": "text"
      },
      "source": [
        "test_1_sent_excluded에 대해 Word2vec을 적용하여 100개의 predicted_songs와 10개의 predicted_tags를 구한 결과를 test_1_sent_excluded에 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2xscW8M9VP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shsh = list(list(filter(isinsonglist, y_1[k])) for k in range(len(y_1)))\n",
        "shshsh =list(list(filter(isintaglist, y_1[k])) for k in range(len(y_1)))\n",
        "for l in range(len(y_1)):\n",
        "  test_1_sent_excluded['predicted_songs'][l] = shsh[l][:100]\n",
        "  test_1_sent_excluded['predicted_tags'][l] = shshsh[l][:10]\n",
        "  \n",
        "test_1_sent_excluded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGsC1xnL8a0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent_excluded = test_1_sent_excluded[['id', 'predicted_songs', 'predicted_tags']]\n",
        "test_1_sent_excluded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJS1HNpZ8rei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent_excluded.to_json('S_result_1.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1dT487f8avh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3KMLo0n8aqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoBWlKOh8amH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}