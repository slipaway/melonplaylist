{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "matrix factorization example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEdS3/k56MYwpZHB3aaOK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slipaway/melonplaylist/blob/%EC%9E%AC%ED%98%84/matrix_factorization_example(%EB%A9%9C%EB%A1%A0%20%EC%95%84%EB%8B%98).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsuzhBS8gwmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class MatrixFactorization():\n",
        "  def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n",
        "    \"\"\"\n",
        "    :param R : rating matrix\n",
        "    :param k : latent parameter\n",
        "    :param learning_rate: alpha on weight update\n",
        "    :param reg_param: beta on weight update\n",
        "    :param epochs: training epochs\n",
        "    :param verobse: print status\n",
        "    \"\"\"\n",
        "\n",
        "    self._R = R\n",
        "    self._num_users, self._num_items = R.shape\n",
        "    self._k = k\n",
        "    self._learning_rate = learning_rate\n",
        "    self._reg_param = reg_param\n",
        "    self._epochs = epochs\n",
        "    self._verbose = verbose\n",
        "\n",
        "\n",
        "  def fit(self):\n",
        "    \"\"\"\n",
        "    training Matrix Factorization : Update matrix latent weight and bias\n",
        "    참고 : self._b에 대한 설명\n",
        "    - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n",
        "    - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌\n",
        "\n",
        "    :return: training_process\n",
        "    \"\"\"\n",
        "\n",
        "    # init latent features\n",
        "    self._P = np.random.normal(size=(self._num_users, self._k))\n",
        "    self._Q = np.random.normal(size=(self._num_items, self._k))\n",
        "\n",
        "    #init biases\n",
        "    self._b_P = np.zeros(self._num_users)\n",
        "    self._b_Q = np.zeros(self._num_items)\n",
        "    self._b = np.mean(self._R[np.where(self._R !=0)])\n",
        "\n",
        "    # train while epoches\n",
        "    self._training_process = []\n",
        "    for epoch in range(self._epochs):\n",
        "\n",
        "      #rating이 존재하는 index를 기준으로 training\n",
        "      for i in range(self._num_users):\n",
        "        for j in range(self._num_items):\n",
        "          if self._R[i, j] > 0:\n",
        "            self.gradient_descent(i, j, self._R[i, j])\n",
        "      cost =self.cost()\n",
        "      self._training_process.append((epoch, cost))\n",
        "\n",
        "      #print status\n",
        "      if self._verbose == True and ((epoch+1)%10==0):\n",
        "        print('iteration: %d ; cost = %.4f' % (epoch+1, cost))\n",
        "    \n",
        "\n",
        "  def cost(self):\n",
        "    \"\"\"\n",
        "    compute root mean square error\n",
        "    :return: rmse cost\n",
        "    \"\"\"\n",
        "\n",
        "    #xi, yi : R[xi, yi]는 nonzero value를 의미한다.\n",
        "    \n",
        "    xi, yi=self._R.nonzero()\n",
        "    predicted = self.get_complete_matrix()\n",
        "    cost = 0\n",
        "    for x, y in zip(xi, yi):\n",
        "      cost += pow(self._R[x, y]-predicted[x, y], 2)\n",
        "    return np.sqrt(cost)/len(xi)\n",
        "\n",
        "\n",
        "  def gradient(self, error, i, j):\n",
        "    \"\"\"\n",
        "    gradient of latent feature for GD\n",
        "\n",
        "    :param error: rating - prediction error\n",
        "    :param i :user index\n",
        "    :param j : item index\n",
        "    :return: gradient of latent feature tuple\n",
        "    \"\"\"\n",
        "    dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n",
        "    dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n",
        "    return dp, dq\n",
        "\n",
        "  def gradient_descent(self, i, j, rating):\n",
        "    # get error\n",
        "    prediction=self.get_prediction(i, j)\n",
        "    error=rating-prediction\n",
        "\n",
        "    #update biases\n",
        "    self._b_P[i] += self._learning_rate * (error - self._reg_param*self._b_P[i])\n",
        "    self._b_Q[j] += self._learning_rate*(error-self._reg_param*self._b_Q[j])\n",
        "\n",
        "    #update latent feature\n",
        "    dp, dq = self.gradient(error, i, j)\n",
        "    self._P[i, :] +=self._learning_rate * dp\n",
        "    self._Q[j, :] +=self._learning_rate * dq\n",
        "\n",
        "  def get_prediction(self, i, j):\n",
        "    \"\"\"\n",
        "    get predicted rating : user_i, item_j\n",
        "    :return: prediction of r_ij\n",
        "    \"\"\"\n",
        "    return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n",
        "\n",
        "  def get_complete_matrix(self):\n",
        "    \"\"\"\n",
        "    computer complete matrix PXQ + P.bias + Q.bias+global bias\n",
        "\n",
        "    - PXQ 행렬에 b_P[:, np.newaxis]를 더하는 것은 각 열마다 bias를 더해주는 것\n",
        "    - b_Q[np.newaxis:, ]를 더하는 것은 각 행마다 bias를 더해주는 것\n",
        "    - b를 더하는 것은 각 element마다 bais를 더해주는 것\n",
        "    - newaxis : 차원을 추가해줌. 1차원인 latent들로 2차원의 R에 행/열 단위 연산을 해주기 위해 차원을 추가하는 것\n",
        "\n",
        "    :return: complete matrix R^\n",
        "    \"\"\"\n",
        "    return self._b+self._b_P[:, np.newaxis]+self._b_Q[np.newaxis:, ]+self._P.dot(self._Q.T)\n",
        "\n",
        "  def print_results(self):\n",
        "    \"\"\"\n",
        "    print fit results\n",
        "    \"\"\"\n",
        "\n",
        "    print('User Latent P:')\n",
        "    print(self._P)\n",
        "    print('Item Latent Q:')\n",
        "    print(self._Q.T)\n",
        "    print('P x Q:')\n",
        "    print(self._P.dot(self._Q.T))\n",
        "    print('bias:')\n",
        "    print(self._b)\n",
        "    print('User Latent bias:')\n",
        "    print(self._b_P)\n",
        "    print('item latent bias:')\n",
        "    print(self._b_Q)\n",
        "    print('Final R matrix:')\n",
        "    print(self.get_complete_matrix())\n",
        "    print('Final RMSE:')\n",
        "    print(self._training_process[self._epochs-1][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90j0v6VAg41q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}