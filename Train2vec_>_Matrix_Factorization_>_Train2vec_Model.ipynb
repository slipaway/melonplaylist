{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train2vec -> Matrix Factorization -> Train2vec Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPx6jxV5GjCktYBwuX27FIW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slipaway/melonplaylist/blob/master/Train2vec_%3E_Matrix_Factorization_%3E_Train2vec_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xglmAVfp0js1",
        "colab_type": "text"
      },
      "source": [
        "train data 전처리 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jId3ZtZ30fb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/kakao/khaiii.git\n",
        "!pip install cmake\n",
        "!mkdir build\n",
        "!cd build && cmake /content/khaiii\n",
        "!cd /content/build/ && make all\n",
        "!cd /content/build/ && make resource\n",
        "!cd /content/build && make install\n",
        "!cd /content/build && make package_python\n",
        "!pip install /content/build/package_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cdfa_ek1WdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz1rF6KS1XXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from datetime import timedelta, datetime\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import *\n",
        "from khaiii import KhaiiiApi\n",
        "from collections import Counter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztkfew2D1bQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder = '/content/gdrive/My Drive/melon'\n",
        "filelist = os.chdir(folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTEkEOOp1irn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre_gn_all=pd.read_json('genre_gn_all.json', typ='series')\n",
        "train=pd.read_json('train.json')\n",
        "song_meta=pd.read_json('song_meta.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3NxXYCb1jmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/slipaway/melonplaylist.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAkLgAJ-1mqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre_gn_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N90_e4n11nXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre_gn_all = pd.DataFrame(genre_gn_all, columns = ['gnr_name']).reset_index().rename(columns = {'index' : 'gnr_code'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xfawjDV1pQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gnr_code=genre_gn_all[genre_gn_all['gnr_code'].str[-2:]=='00']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnSAVY7b1vjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#대장르 장르코드 리스트\n",
        "gnr_code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN__9R7s1yV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#곡 아이디(id)와 대분류 장르코드 리스트(song_gn_gnr_basket) 추출\n",
        "\n",
        "song_gnr_map = song_meta.loc[:, ['id', 'song_gn_gnr_basket']]\n",
        "\n",
        "#unnest song_gn_gnr_basket\n",
        "song_gnr_map_unnest = np.dstack(\n",
        "    (\n",
        "        np.repeat(song_gnr_map.id.values, list(map(len, song_gnr_map.song_gn_gnr_basket))),\n",
        "        np.concatenate(song_gnr_map.song_gn_gnr_basket.values)\n",
        "    )\n",
        ")\n",
        "\n",
        "#unnested 데이터프레임 생성 : song_gnr_map\n",
        "\n",
        "song_gnr_map = pd.DataFrame(data = song_gnr_map_unnest[0], columns=song_gnr_map.columns)\n",
        "song_gnr_map['id']=song_gnr_map['id'].astype(str)\n",
        "song_gnr_map.rename(columns={'id' : 'song_id', 'song_gn_gnr_basket' : 'gnr_code'}, inplace=True)\n",
        "\n",
        "#unnest 객체 제거\n",
        "del song_gnr_map_unnest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcJCfJbb160g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plylst_date = train[['updt_date', 'songs']]\n",
        "plylst_date_unnest = np.dstack(\n",
        "    (\n",
        "        np.repeat(plylst_date.updt_date.values, list(map(len, plylst_date.songs))), \n",
        "        np.concatenate(plylst_date.songs.values)\n",
        "    )\n",
        ")\n",
        "plylst_date = pd.DataFrame(data=plylst_date_unnest[0], columns = plylst_date.columns)\n",
        "\n",
        "del plylst_date_unnest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx_vSQIU19aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plylst_date[\"updt_date\"] = plylst_date[\"updt_date\"].map(lambda x: x[:4] + x[5:7] + x[8:10])\n",
        "song_date = song_meta[[\"id\",\"issue_date\"]]\n",
        "plylst_song_date = pd.merge(plylst_date, song_date, left_on=\"songs\", right_on=\"id\", how='left')\n",
        "plylst_song_date[\"issue_date\"] = plylst_song_date[\"issue_date\"].astype(str)\n",
        "\n",
        "plylst_song_date[\"strange\"] = plylst_song_date[\"updt_date\"] < plylst_song_date[\"issue_date\"]\n",
        "strange_songs = plylst_song_date[plylst_song_date[\"strange\"] == True].drop('songs', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1g3ldno1iUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDdFx2gJ2B8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs=strange_songs.drop_duplicates(['id'])\n",
        "len(strange_songs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byM7JEbi2EM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs_list=strange_songs['id'].values.astype(str)\n",
        "strange_song_gnr_map=song_gnr_map[song_gnr_map['song_id'].isin(strange_songs_list)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa6qY6rw2HRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(strange_songs_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkMgd5I-2JPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "song_gnr_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QWC_Tuc2Ln9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_song_gnr_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2jFNT7q2OL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_cnt=strange_song_gnr_map.groupby('gnr_code').count()\n",
        "strange_cnt=pd.merge(strange_cnt, gnr_code, on='gnr_code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsVZMx7o2Q9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCa3NF4j2T82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs=strange_songs.drop_duplicates(['id'])\n",
        "print(strange_songs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0VogwC-2WXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_id=strange_songs.loc[:, 'id']\n",
        "strange_id\n",
        "strange_id_list=list(strange_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N0ZWDZl2X2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_id=train.loc[:, 'id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6CbJD1A2Zry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "song_meta_id=song_meta.loc[:, 'id']\n",
        "print(song_meta_id.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e53Aw7zD2cGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strange_songs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNF3g5HW2fKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트 아이디(id)와 수록곡(songs) 추출\n",
        "plylst_song_map=train[['id', 'songs']]\n",
        "\n",
        "#unnest songs\n",
        "\n",
        "plylst_song_map_unnest=np.dstack(\n",
        "    (\n",
        "        np.repeat(plylst_song_map.id.values, list(map(len, plylst_song_map.songs))),\n",
        "        np.concatenate(plylst_song_map.songs.values)\n",
        "    )\n",
        ")\n",
        "\n",
        "#unnested 데이터프레임 생성 : plylst_song_map\n",
        "plylst_song_map=pd.DataFrame(data=plylst_song_map_unnest[0], columns=plylst_song_map.columns)\n",
        "plylst_song_map['id']=plylst_song_map['id'].astype(str)\n",
        "plylst_song_map['songs']=plylst_song_map['songs'].astype(str)\n",
        "\n",
        "del plylst_song_map_unnest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-p-DHTe2jhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트에 수록된 곡의 총 개수(중복 포함)\n",
        "\n",
        "len(plylst_song_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6syEPt72lty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트에 strange 곡 제거한 후, 수록된 곡의 총 개수(중복 포함)\n",
        "\n",
        "plylst_song_map_remove=plylst_song_map[np.logical_not(plylst_song_map.songs.isin(strange_songs_list))]\n",
        "plylst_song_map_remove.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Pgxx8wE2nJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plylst_song_map_remove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKnQ8M-82q81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train set에 새로운 column : remove_songs 생성\n",
        "#remove_songs : strange 곡 제거한 후 다시 만든 플레이리스트\n",
        "\n",
        "plylst_song_group=plylst_song_map_remove.groupby('id')['songs'].apply(list).reset_index(name='remove_songs')\n",
        "train['id']=train['id'].astype(str)\n",
        "train_remove = pd.merge(train, plylst_song_group)\n",
        "\n",
        "train_remove.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO6NctFQ2vPv",
        "colab_type": "text"
      },
      "source": [
        "train data에서 tag와 playlist title을 형태소 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15xQE9iz298a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def re_sub(series: pd.Series) -> pd.Series:\n",
        "  series = series.str.replace(pat=r'[ㄱ-ㅎ]', repl=r'', regex=True) #ㅋ 제거용\n",
        "  series = series.str.replace(pat=r'[^\\w\\s]', repl=r'', regex=True) #특수문자 제거용\n",
        "  series = series.str.replace(pat=r'[ ]{2,}', repl=r' ', regex=True) #공백 제거\n",
        "  series = series.str.replace(pat=r'[\\u3000]+', repl=r'', regex=True) #u3000제거\n",
        "  return series\n",
        "\n",
        "def flatten(list_of_list : List) -> List:\n",
        "  flatten = [j for i in list_of_list for j in i]\n",
        "  return flatten\n",
        "\n",
        "def get_token(title: str, tokenizer)-> List[Tuple]:\n",
        "\n",
        "  if len(title)==0 or title== ' ' : #제목이 공백인 경우 tokenizer 에러 발생\n",
        "    return []\n",
        "  \n",
        "  result = tokenizer.analyze(title)\n",
        "  result = [(morph.lex, morph.tag) for split in result for morph in split.morphs] # (형태소, 품사) 튜플의 리스트\n",
        "  return result\n",
        "\n",
        "def get_all_tags(df) -> List:\n",
        "  tag_list = df['tags'].values.tolist()\n",
        "  tag_list=flatten(tag_list)\n",
        "  return tag_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNBo5sw13App",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = KhaiiiApi()\n",
        "all_tag = get_all_tags(train)\n",
        "token_tag = [get_token(x, tokenizer) for x in all_tag] #태그를 형태소 분석"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHRcQ8wL3C3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_tag[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMtXOrmr3EwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_itself=list(filter(lambda x: len(x)==1, token_tag)) # 태그 자체가 형태소여서 분리되지 않는 태그만 고름\n",
        "token_itself=flatten(token_itself)\n",
        "flatten_token=flatten(token_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFTi1nFe3RHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('%-23s'%'# of original tag is', f'{len(all_tag):8,}')\n",
        "print('%-23s'%'# of morpheme itself is', f'{len(token_itself):8,}')\n",
        "print('%-23s'%'# of total token is', f'{len(flatten_token):8,}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_Z-o28A3TEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#counting part of speech (pos)\n",
        "pos=[x[1] for x in token_itself]\n",
        "pos_count=Counter(pos)\n",
        "popular_pos = pos_count.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6VlV8a83VTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tag 분류표\n",
        "objects=[x[0] for x in popular_pos]\n",
        "y_pos=np.arange(len(objects))\n",
        "performance=[x[1] for x in popular_pos]\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Usage')\n",
        "plt.title('Part of Speech - Tags')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWYcmBz73Z2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트 제목 형태소 분석\n",
        "\n",
        "train['plylst_title']=re_sub(train['plylst_title'])\n",
        "train.loc[:, 'ply_token']=train['plylst_title'].map(lambda x: get_token(x, tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjJljL8a3bGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "using_pos=['NNG', 'SL', 'NNP', 'MAG', 'SN'] #일반 명사, 외국어, 고유 명사, 일반 부사, 숫자\n",
        "train['ply_token']=train['ply_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoCwCoXW3ftY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_tag=set(token_itself)\n",
        "unique_word=[x[0] for x in unique_tag]\n",
        "\n",
        "#train 데이터의 plylst title 형태소 분리"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-OKcii83i20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#정답 tag에 나온 형태소만 남겨두기\n",
        "train['ply_token']=train['ply_token'].map(lambda x: list(filter(lambda x: x[0] in unique_word, x)))\n",
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65WJh6A53ok8",
        "colab_type": "text"
      },
      "source": [
        "형태소 분석한 tag, playlist title과 날짜가 맞지 않는 노래들을 제외한 목록이 추가된 train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKUI5Al03rA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_eda = pd.merge(train, plylst_song_group)\n",
        "train_eda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3StZ3jd3vv0",
        "colab_type": "text"
      },
      "source": [
        "word2vec 학습을 위해 사용할 데이터들만 모아 놓은 dataframe 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOrODbqE3s4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wv_sent = train_eda[['ply_token', 'tags', 'remove_songs']]\n",
        "wv_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFXxyBxg3-kc",
        "colab_type": "text"
      },
      "source": [
        "플레이리스트 제목을 형태소 분해한 데이터에서 품사정보를 제외"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGUIhuxf2jQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word2vec 이용을 위해 ply_token data를 전처리\n",
        "z = []\n",
        "y = []\n",
        "for i in range(len(wv_sent['ply_token'])):\n",
        "  z = []\n",
        "  for j in range(len(wv_sent['ply_token'][i])):\n",
        "    x = wv_sent['ply_token'][i][j]\n",
        "    x = list(x)\n",
        "    del x[1]\n",
        "    z.insert(j,\"\".join(x))\n",
        "  y.insert(i,z)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78HkrjGg4E0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wv_sent['ply_token_new'] = y\n",
        "wv_sent = wv_sent[['ply_token_new', 'tags', 'remove_songs']]\n",
        "wv_sent # Word2vec 이용을 위해 필요한 data들만 모아둔 dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmy0PRW54LCa",
        "colab_type": "text"
      },
      "source": [
        "train data에 존재하는 플레이리스트 제목을 형태소 분해한 것들의 리스트\n",
        "& train data에 존재하는 태그들을 형태소 분해한 것들의 리스트\n",
        "& train data에 존재하는 노래들의 리스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_2wlq3e4MP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_token_list = np.dstack(\n",
        "    (\n",
        "        np.concatenate(wv_sent.ply_token_new.values)\n",
        "    )\n",
        ")\n",
        "train_token_list = pd.DataFrame(train_token_list[0][0], columns = ['token'])\n",
        "train_token_list = list(train_token_list['token']) # train data에 존재하는 모든 플레이리스트 제목 형태소들의 list\n",
        "\n",
        "train_tag_list = np.dstack(\n",
        "    (\n",
        "        np.concatenate(wv_sent.tags.values)\n",
        "    )\n",
        ")\n",
        "train_tag_list = pd.DataFrame(train_tag_list[0][0], columns = ['tag'])\n",
        "train_tag_list = list(train_tag_list['tag']) # train data에 존재하는 모든 태그들의 list\n",
        "\n",
        "train_song_list = np.dstack(\n",
        "    (\n",
        "        np.concatenate(wv_sent.remove_songs.values)\n",
        "    )\n",
        ")\n",
        "train_song_list = pd.DataFrame(train_song_list[0][0], columns = ['song'])\n",
        "train_song_list = list(train_song_list['song']) # train data에 존재하는 노래들의 list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68A7U8EX4Ew1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_token_list_unduplicated = list(set(train_token_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgjB6lV4VCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tag_list_unduplicated = list(set(train_tag_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vNFk96R4Wsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_song_list_unduplicated = list(set(train_song_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohi-vJu28APp",
        "colab_type": "text"
      },
      "source": [
        "Word2vec 사용을 위해 플레이리스트 별로 제목의 형태소와 태그, 곡 id를 한 문장으로 갖도록 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyEQu5WL8FjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word2vec 학습을 위해 플레이리스트 별로 제목의 형태소와 태그, 곡 id를 한 문장으로 갖도록 전처리\n",
        "w = []\n",
        "for i in range(len(wv_sent)):\n",
        "  w.insert(i,wv_sent['ply_token_new'][i] + wv_sent['tags'][i] + wv_sent['remove_songs'][i])\n",
        "train_eda_fin = w\n",
        "train_eda_fin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ian5HSOa8SMq",
        "colab_type": "text"
      },
      "source": [
        "word2vec 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4A4vwOc8I10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "class EpochSaver(CallbackAny2Vec):\n",
        "     '''Callback to save model after each epoch.'''\n",
        "\n",
        "     def __init__(self, path_prefix):\n",
        "         self.path_prefix = path_prefix\n",
        "         self.epoch = 0\n",
        "\n",
        "     def on_epoch_end(self, model):\n",
        "         output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n",
        "         model.save(output_path)\n",
        "         self.epoch += 1\n",
        "\n",
        "class EpochLogger(CallbackAny2Vec):\n",
        "     def __init__(self):\n",
        "         self.epoch = 0\n",
        "\n",
        "     def on_epoch_begin(self, model):\n",
        "         print(\"Epoch #{} start\".format(self.epoch))\n",
        "\n",
        "     def on_epoch_end(self, model):\n",
        "         print(\"Epoch #{} end\".format(self.epoch))\n",
        "         self.epoch += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zduxj5H4EtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "epoch_logger = EpochLogger()\n",
        "wv_model = Word2Vec(train_eda_fin, window = 10, min_count=7, workers=6, size = 400, iter=50, sg=1, callbacks=[epoch_logger])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaKaO4Wy4Eow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_list = wv_model.wv.vocab.keys()\n",
        "# word2vec에 의해 학습된 것들(플레이 리스트 제목이 형태소로 분해된 것들 & 태그 & 곡 id)의 리스트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSxZqIXT8gIJ",
        "colab_type": "text"
      },
      "source": [
        "test data 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8f2wgSz8a5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=pd.read_json('test.json')\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVwJWIxp8lGE",
        "colab_type": "text"
      },
      "source": [
        "test data 중에서 일부 곡과 일부 태그가 존재하는 경우를 제외한 data set 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RMwgEga8mJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "me = []\n",
        "for i in range(len(test['id'])):\n",
        "  if len(test['plylst_title'][i]) == 0 and len(test['tags'][i]) != 0 and len(test['songs'][i]) !=0:\n",
        "    me.append(i)\n",
        "  else:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4CDfJdM8tmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = test.drop(me, axis = 0)\n",
        "test = test.reset_index(drop=True)\n",
        "test # 곡과 태그만 존재하는 플레이리스트를 제외한 나머지 플레이리스트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Suud8583DO",
        "colab_type": "text"
      },
      "source": [
        "위의 결과로 얻은 데이터셋 중에서 곡만 존재하는 플레이리스트를 제외한 데이터셋 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCyzWtuf8wLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mel = []\n",
        "for i in range(len(test['id'])):\n",
        "  if len(test['plylst_title'][i]) == 0 and len(test['tags'][i]) == 0 and len(test['songs'][i]) !=0:\n",
        "    mel.append(i)\n",
        "  else:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LygJHDbK81he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1 = test.drop(mel, axis = 0)\n",
        "test_1 = test_1.reset_index(drop=True)\n",
        "test_1 # 곡과 태그만 존재하는 플레이리스트를 제외한 나머지 플레이리스트 중에서 곡만 존재하는 플레이리스트를 제외 -> 제목+태그 or 제목만 있는 경우"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y17j7N99L0H",
        "colab_type": "text"
      },
      "source": [
        "곡과 태그만 존재하는 플레이리스트를 제외한 나머지 플레이리스트 중에서 곡만 존재하는 플레이리스트만 존재하는 데이터셋 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47TKsxCM9ITh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_2 = test.loc[mel]\n",
        "test_2 = test_2.reset_index(drop=True)\n",
        "test_2 # 곡과 태그만 존재하는 플레이리스트를 제외한 나머지 플레이리스트 중에서 곡만 존재하는 플레이리스트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1vPHMd29ZyB",
        "colab_type": "text"
      },
      "source": [
        "test_1 data의 제목과 태그에 대해 형태소 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqWHIu9J9fiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def re_sub(series: pd.Series) -> pd.Series:\n",
        "  series = series.str.replace(pat=r'[ㄱ-ㅎ]', repl=r'', regex=True) #ㅋ 제거용\n",
        "  series = series.str.replace(pat=r'[^\\w\\s]', repl=r'', regex=True) #특수문자 제거용\n",
        "  series = series.str.replace(pat=r'[ ]{2,}', repl=r' ', regex=True) #공백 제거\n",
        "  series = series.str.replace(pat=r'[\\u3000]+', repl=r'', regex=True) #u3000제거\n",
        "  return series\n",
        "\n",
        "def flatten(list_of_list : List) -> List:\n",
        "  flatten = [j for i in list_of_list for j in i]\n",
        "  return flatten\n",
        "\n",
        "def get_token(title: str, tokenizer)-> List[Tuple]:\n",
        "\n",
        "  if len(title)==0 or title== ' ' : #제목이 공백인 경우 tokenizer 에러 발생\n",
        "    return []\n",
        "  \n",
        "  result = tokenizer.analyze(title)\n",
        "  result = [(morph.lex, morph.tag) for split in result for morph in split.morphs] # (형태소, 품사) 튜플의 리스트\n",
        "  return result\n",
        "\n",
        "def get_all_tags(df) -> List:\n",
        "  tag_list = df['tags'].values.tolist()\n",
        "  tag_list=flatten(tag_list)\n",
        "  return tag_list\n",
        "\n",
        "tokenizer = KhaiiiApi()\n",
        "all_tag = get_all_tags(test_1)\n",
        "token_tag = [get_token(x, tokenizer) for x in all_tag] #태그를 형태소 분석\n",
        "\n",
        "token_tag[:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb-W5PUu9g8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_itself=list(filter(lambda x: len(x)==1, token_tag)) # 태그 자체가 형태소여서 분리되지 않는 태그만 고름\n",
        "token_itself=flatten(token_itself)\n",
        "flatten_token=flatten(token_tag)\n",
        "print('%-23s'%'# of original tag is', f'{len(all_tag):8,}')\n",
        "print('%-23s'%'# of morpheme itself is', f'{len(token_itself):8,}')\n",
        "print('%-23s'%'# of total token is', f'{len(flatten_token):8,}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5swouqn9l5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#counting part of speech (pos)\n",
        "pos=[x[1] for x in token_itself]\n",
        "pos_count=Counter(pos)\n",
        "popular_pos = pos_count.most_common()\n",
        "#tag 분류표\n",
        "objects=[x[0] for x in popular_pos]\n",
        "y_pos=np.arange(len(objects))\n",
        "performance=[x[1] for x in popular_pos]\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Usage')\n",
        "plt.title('Part of Speech - Tags')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EVQvJNy9nJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#플레이리스트 제목 형태소 분석\n",
        "test_1['plylst_title']=re_sub(test_1['plylst_title'])\n",
        "test_1.loc[:, 'ply_token']=test_1['plylst_title'].map(lambda x: get_token(x, tokenizer))\n",
        "using_pos=['NNG', 'SL', 'NNP', 'MAG', 'SN'] #일반 명사, 외국어, 고유 명사, 일반 부사, 숫자\n",
        "test_1['ply_token']=test_1['ply_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n",
        "unique_tag=set(token_itself)\n",
        "unique_word=[x[0] for x in unique_tag]\n",
        "#정답 tag에 나온 형태소만 남겨두기\n",
        "test_1['ply_token']=test_1['ply_token'].map(lambda x: list(filter(lambda x: x[0] in unique_word, x)))\n",
        "test_1.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov_T_1rb-XKe",
        "colab_type": "text"
      },
      "source": [
        "test_1 data에서 형태소 분석한 결과를 적용시킨 data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiXhzgaf-ZP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCqAQ2iP-dsn",
        "colab_type": "text"
      },
      "source": [
        "test_2 data에서 앨범 최종 수정일 이후에 발매된 음악이 포함된 경우를 제외하는 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf9D7O5q9jsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_2_song=test_2[['id', 'songs']]\n",
        "\n",
        "filter2=test_2_song['songs'].astype('str')=='[]'\n",
        "test_2_song_remove=test_2_song.loc[~filter2, :]\n",
        "\n",
        "test_2_song_remove_unnest=np.dstack(\n",
        "    (\n",
        "        np.repeat(test_2_song_remove.id.values, list(map(len, test_2_song_remove.songs))),\n",
        "        np.concatenate(test_2_song_remove.songs.values)\n",
        "    )\n",
        "  \n",
        ")\n",
        "\n",
        "test_2_song_remove=pd.DataFrame(data=test_2_song_remove_unnest[0], columns=test_2_song_remove.columns)\n",
        "test_2_song_remove['id']=test_2_song_remove['id'].astype('str')\n",
        "test_2_song_remove['songs']=test_2_song_remove['songs'].astype('str')\n",
        "\n",
        "del test_2_song_remove_unnest\n",
        "\n",
        "test_2_song_remove=test_2_song_remove.groupby('id')['songs'].apply(list).reset_index(name='remove_songs')\n",
        "\n",
        "test_2_remove=test_2[~filter2]\n",
        "test_2_remove['id']=test_2_remove['id'].astype('str')\n",
        "test_2_remove=pd.merge(test_2_remove, test_2_song_remove)\n",
        "\n",
        "test_2_remove\n",
        "\n",
        "test_2_not_remove=test_2[filter2]\n",
        "\n",
        "test_2_not_remove\n",
        "\n",
        "test_2_remove['songs']=test_2_remove['remove_songs']\n",
        "test_2_remove=test_2_remove.drop('remove_songs', axis=1)\n",
        "test_2_fixed=pd.concat([test_2_remove, test_2_not_remove])\n",
        "test_2_fixed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFUfppd3-klo",
        "colab_type": "text"
      },
      "source": [
        "전처리된 test_1 data에 대해 word2vec 적용을 위해 필요한 data들만 추출한 dataframe 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IGbDPTE-jfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent = test_1[['id', 'ply_token', 'tags']]\n",
        "test_1_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM0WsoIe-oAO",
        "colab_type": "text"
      },
      "source": [
        "전처리된 test_2 data에 대해 word2vec 적용을 위해 필요한 data들만 추출한 dataframe 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2xscW8M9VP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_2_sent = test_2_fixed[['id', 'songs']]\n",
        "test_2_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJS1HNpZ8rei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent = pd.DataFrame(test_1_sent)\n",
        "test_2_sent = pd.DataFrame(test_2_sent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iddWvsGb-vDI",
        "colab_type": "text"
      },
      "source": [
        "전처리된 test_1 data 중 플레이리스트 제목을 형태소 분해한 data에서 품사정보를 삭제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGsC1xnL8a0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word2vec 이용을 위해 ply_token data를 전처리\n",
        "z = []\n",
        "y = []\n",
        "for i in range(len(test_1_sent['ply_token'])):\n",
        "  z = []\n",
        "  if len(test_1_sent['ply_token'][i]) == 0:\n",
        "    x = test_1_sent['ply_token'][i]\n",
        "    z = x\n",
        "  else:\n",
        "    for j in range(len(test_1_sent['ply_token'][i])):\n",
        "     x = test_1_sent['ply_token'][i][j]\n",
        "     z.insert(j,x[0])\n",
        "  y.insert(i,z)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-DDAUAl-zOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent['ply_token'] = y\n",
        "test_1_sent # Word2vec 이용을 위해 필요한 data들만 모아둔 dataframe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOZ4TqLO-zGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_1_sent에서 학습된 리스트에 존재하지 않는 ply_token을 제외함 (Word2vec 모델 적용을 위함)\n",
        "for i in range(len(test_1_sent['ply_token'])):\n",
        "  if len(test_1_sent['ply_token'][i]) == 0:\n",
        "    test_1_sent['ply_token'][i] = test_1_sent['ply_token'][i]\n",
        "  else:\n",
        "    a = pd.DataFrame(test_1_sent['ply_token'][i], columns = ['token'])\n",
        "    a['logical'] = a.isin(learning_list).astype(str)\n",
        "    b = a['logical'] == 'True' \n",
        "    if b.sum() == 0:\n",
        "      test_1_sent['ply_token'][i] = []\n",
        "    else:\n",
        "      c = c = a[b].drop('logical',axis=1)\n",
        "      c = c.token.tolist()\n",
        "      test_1_sent['ply_token'][i] = c\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0lL6JzB-zAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_1_sent에서 학습된 리스트에 존재하지 않는 tags를 제외함 (Word2vec 모델 적용을 위함)\n",
        "for i in range(len(test_1_sent['tags'])):\n",
        "  if len(test_1_sent['tags'][i]) == 0:\n",
        "    test_1_sent['tags'][i] = test_1_sent['tags'][i]\n",
        "  else:\n",
        "    a = pd.DataFrame(test_1_sent['tags'][i], columns = ['tag'])\n",
        "    a['logical'] = a.isin(learning_list).astype(str)\n",
        "    b = a['logical'] == 'True' \n",
        "    if b.sum() == 0:\n",
        "      test_1_sent['tags'][i] = []\n",
        "    else:\n",
        "      c = c = a[b].drop('logical',axis=1)\n",
        "      c = c.tag.tolist()\n",
        "      test_1_sent['tags'][i] = c\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEZiCxQY-y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_2_sent에서 학습된 리스트에 존재하지 않는 songs를 제외함 (Word2vec 모델 적용을 위함)\n",
        "for i in range(len(test_2_sent['songs'])):\n",
        "  if len(test_2_sent['songs'][i]) == 0:\n",
        "    test_2_sent['songs'][i] = test_2_sent['songs'][i]\n",
        "  else:\n",
        "    a = pd.DataFrame(test_2_sent['songs'][i], columns = ['song'])\n",
        "    a['logical'] = a.isin(learning_list).astype(str)\n",
        "    b = a['logical'] == 'True' \n",
        "    if b.sum() == 0:\n",
        "      test_2_sent['songs'][i] = []\n",
        "    else:\n",
        "      c = c = a[b].drop('logical',axis=1)\n",
        "      c = c.song.tolist()\n",
        "      test_2_sent['songs'][i] = c\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIdubBtL_OLA",
        "colab_type": "text"
      },
      "source": [
        "word2vec 적용을 위해 학습된 리스트에 존재하지 않는 data들을 제외하는 과정에서 input이 모두 사라져버린 data set들을 추출해내는 과정 -> Matrix Factorization 모델을 통해 곡 또는 태그를 채워서 다시 Word2vec 모델을 적용할 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9QVWGSO-y0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m1 = []\n",
        "for i in range(len(test_1_sent['id'])):\n",
        "  if len(test_1_sent['ply_token'][i]) == 0 and len(test_1_sent['tags'][i]) == 0:\n",
        "    m1.append(i)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "test_1_sent_zerolen = test_1_sent.loc[m1]\n",
        "test_1_sent_zerolen = test_1_sent_zerolen.reset_index(drop=True)\n",
        "test_1_sent_zerolen # test_1_sent 중 전처리결과 token, tag, song 모두 하나도 남지 않게 된 플레이리스트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1dT487f8avh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m2 = []\n",
        "for i in range(len(test_2_sent['id'])):\n",
        "  if len(test_2_sent['songs'][i]) == 0:\n",
        "    m2.append(i)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "test_2_sent_zerolen = test_2_sent.loc[m2]\n",
        "test_2_sent_zerolen = test_2_sent_zerolen.reset_index(drop=True)\n",
        "test_2_sent_zerolen # test_2_sent 중 전처리결과 token, tag, song 모두 하나도 남지 않게 된 플레이리스트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ZfII7C_rIV",
        "colab_type": "text"
      },
      "source": [
        "Matrix Factorization 모델 적용을 위해 저장 후 전송"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3KMLo0n8aqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1_sent_zerolen.to_json('빈태그 채우기.json')\n",
        "test_2_sent_zerolen.to_json('빈노래 채우기.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sLtE_GO__Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs217zR9__Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWFUyDc4__AA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu63hKbT_-44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvavVLLC_-xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QpvspaF_-qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80rc3mf6_-jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp6Dis3A_-dB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jW1xNcL_-Vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6YrZNVdAAr4",
        "colab_type": "text"
      },
      "source": [
        "기존 test 데이터셋에 제목과 태그가 존재했으나 Word2vec 이용을 위한 전처리 과정에서 제목과 태그가 사라져 Word2vec 모델 적용이 불가했던 데이터셋을 Matrix Factorization 모델을 통해 기존 데이터셋의 태그를 이용하여 예측태그를 형성 -> test_3_sent\n",
        "기존 test 데이터셋에 곡 id가 존재했으나 Word2vec 이용을 위한 전처리 과정에서 곡 id가 사라져 Word2vec 모델 적용이 불가했던 데이터셋을 Matrix Factorization 모델을 통해 기존 데이터셋의 곡 id를 이용하여 예측 곡 id를 형성 -> test_4_sent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X6sfzte_-PO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_3_sent = pd.read_json('빈태그 채우기.json')\n",
        "test_4_sent = pd.read_json('빈노래 채우기.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RytGLil_-I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_3_sent에서 학습된 리스트에 존재하지 않는 tags를 제외함 (Word2vec 모델 적용을 위함)\n",
        "for i in range(len(test_3_sent['predicted_tag'])):\n",
        "    a = pd.DataFrame(test_3_sent['predicted_tag'][i], columns = ['predicted_tag'])\n",
        "    a['logical'] = a.isin(learning_list).astype(str)\n",
        "    b = a['logical'] == 'True' \n",
        "    if b.sum() == 0:\n",
        "      test_3_sent['predicted_tag'][i] = []\n",
        "    else:\n",
        "      c = c = a[b].drop('logical',axis=1)\n",
        "      c = c.predicted_tag.tolist()\n",
        "      test_3_sent['predicted_tag'][i] = c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXBu8ZUV_-B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_4_sent에서 학습된 리스트에 존재하지 않는 songs를 제외함 (Word2vec 모델 적용을 위함)\n",
        "for i in range(len(test_4_sent['predicted_songs'])):\n",
        "  a = pd.DataFrame(test_4_sent['predicted_songs'][i], columns = ['predicted_songs'])\n",
        "  a['logical'] = a.isin(learning_list).astype(str)\n",
        "  b = a['logical'] == 'True' \n",
        "  if b.sum() == 0:\n",
        "    test_4_sent['predicted_songs'][i] = []\n",
        "  else:\n",
        "    c = c = a[b].drop('logical',axis=1)\n",
        "    c = c.predicted_songs.tolist()\n",
        "    test_4_sent['predicted_songs'][i] = c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoBWlKOh8amH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_4_sent\n",
        "length = test_4_sent.shape[0]\n",
        "\n",
        "index = []\n",
        "for i in range(length):\n",
        "  if len(test_4_sent['predicted_songs'][i]) != 0:\n",
        "    index.append(i)\n",
        "\n",
        "index\n",
        "# index = [] -> test_4_sent에는 learning list에 포함된 songs가 존재하지 않아서 Word2vec 모델 적용 불가능 -> empty case로 다룸"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R36p14XHBEHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word2vec 사용을 위해 플레이리스트 별로 태그를 한 문장으로 갖도록 test_3_sent_excluded를 전처리\n",
        "w = []\n",
        "for i in range(len(test_3_sent)):\n",
        "  w.insert(i,test_3_sent['predicted_tag'][i])\n",
        "test_3_input_excluded = w\n",
        "test_3_input_excluded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcuTDr9hBQUE",
        "colab_type": "text"
      },
      "source": [
        "test_3_input_expected data에 대해 Word2vec 모델을 이용하여 input과 유사한 output을 500개 뽑아내는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swog5JamBKX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similar_1_list(x):\n",
        "  aa = [wv_model.wv.most_similar(positive = x[i], topn = 500) for i in range(len(x))]\n",
        "  return aa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEeYCHj1BKMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xxxx_3 = similar_1_list(test_3_input_excluded)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnG_AhriBXW0",
        "colab_type": "text"
      },
      "source": [
        "word2vec model을 적용한 결과에서 유사도 data를 제외하고 유사한 data만을 남기는 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h66GEB5RBKDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = []\n",
        "y_3 = []\n",
        "for i in range(len(xxxx_3)):\n",
        "  z = []\n",
        "  for j in range(len(xxxx_3[i])):\n",
        "    x5_3 = xxxx_3[i][j]\n",
        "    z.insert(j,x5_3[0])\n",
        "  y_3.insert(i,z)\n",
        "y_3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38CpBHJcBc3K",
        "colab_type": "text"
      },
      "source": [
        "test_3_sent에 predicted_songs column 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nSQYS1mBJ5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_songs = []\n",
        "for i in range(len(test_3_sent)):\n",
        "  predicted_songs.append([])\n",
        "\n",
        "test_3_sent['predicted_songs'] = predicted_songs\n",
        "test_3_sent\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ20owAqBEti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shsh = list(list(filter(isinsonglist, y_3[k])) for k in range(len(y_3)))\n",
        "for l in range(len(y_3)):\n",
        "  test_3_sent['predicted_songs'][l] = shsh[l][:100]\n",
        "  \n",
        "test_3_sent\n",
        "# 원래 test data set에서 제목과 일부 태그가 존재하는 플레이리스트였으나, 모델 적용을 위해 전처리했을 때 태그와 제목 형태소가 learning list에 속하지 않아 모델 적용이 불가능했던 data set.\n",
        "# word2vec 모델 적용을 위해 matrix factorization method를 이용하여 일부 태그를 바탕으로 10개의 태그를 예측하였고, 10개의 예측 태그를 이용하여 word2vec 모델을 통해 100개의 예측 곡 id를 추출함."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKrREVwoBqUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_3_sent.to_json('MF+Word2vec으로 곡 예측.json')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}